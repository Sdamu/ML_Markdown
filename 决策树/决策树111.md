决策树还表示给定特征条件下类的条件概率分布。这一条件概率分布定义在特征空间的一个划分上。将特征空间划分为互不相交的单元或是区域，并在每个单元定义一个类的概率分布就构成了一个条件概率分布。

决策树中的一条路径对应于划分中的一个单元。决策树所表示的条件概率分布由各个单元给定条件下类的条件概率分布组成。

假设 X 表示特征的随机变量，Y 是表示类的随机变量，那么这个条件概率根部可以表示为 P(Y|X)。X取决于给定划分下的集合，Y 取决于类的集合。各个叶节点（单元）上的条件概率旺旺偏向某一个类，也就是属于某一类的概率比较大。决策树进行分类的时候就是将该节点的实例强行分到条件概率大的那一类去。

我们选择的条件概率模型应该不仅仅对训练数据有很好的拟合，而且对未知数据也应该有很好的预测

决策树学习用损失函数表示这一目标：

> 决策树学习的损失函数通常是正则化的极大似然函数。

决策树的学习策略是 **以损失函数为目标函数的最小化问题**。 



采用常规方法进行构建的决策树可能会对训练数据有很好的分类效果，但是对未知的测试数据却不一定能起到相似的作用，也就是说很容易发生过拟合现象。因此我们需要对已经生成的树进行 **剪枝处理**，目的是让我们的决策树变得简单一些。也就是去掉过分细分的节点。



由于决策树表示一个条件概率分布，所以深浅不同的决策树对应着不同复杂度的概率模型 。决策树的生成对应于模型的局部选择，决策树的剪枝对应于模型的全局选择。



设有随机变量 $(X,Y)$，联合概率密度分布为：
$$
P(X=x_i,Y=y_i)=p_{ij},\ \ \ i=1,2,...,m
$$
条件熵 $H(Y|X)$ 表示在已知随机变量 $X$ 的条件下随机变量 $Y$ 的不确定性。

随机变量 $X$ 给定的条件下随机变量 $Y$ 的条件上(conditional entropy)$\ H(Y|X)$，定义为 $X$ 给定的条件下 $Y$ 的条件概率分布的熵对 $X$ 的数学期望
$$
H(Y|X)=\sum_{i=1}^{n}p_iH(Y|X=x_i)
$$

## 信息增益

特征 $A$ 对训练数据集$D$ 的信息增益 $g(D,A)$，定义为集合 $D$ 的经验熵 $H(D)$ 与特征$A$ 给定的条件下 $D$

 的经验条件熵$H(D|A)$ 之差。也就是
$$
g(D,A)=H(D)-H(D|A)
$$
一般的，熵 $H(Y)$ 与条件熵 $H(D|A)$ 之差称之为 互信息（mutual information)。决策树学习中的信息增益等价于训练数据集中类别与特征的互信息。



## 信息增益比

以信息增益作为划分训练数据集的特征，存在偏向于选择取值较多的特征的问题。使用信息赠一比（information gain ratio) 可以对这一问题进行校正。定义如下所示：

特征 $A$ 对训练数据集 $D$ 的信息增益比 $g_R(D,A)$ 定义为其信息增益 $g(D,A)$ 与训练数据集 $D$ 关于特征 $A$ 的值的熵 $H_A(D)$ 之比。也就是：
$$
g_R(D,A)=\frac{g(D,A)}{H_A(D)}
$$
其中，$H_A(D)=-\sum_{i=1}^{n}\frac{|D_i|}{|D|}log_2\frac{|D_i|}{|D|}$

## 决策树的剪枝

剪枝就是在已经生成的树上裁减掉一些子树或者是叶节点，并将其根节点或父节点作为新的叶节点，从而简化分类树的模型。

决策树的剪枝往往通过极小化决策树**整体**的损失函数（loss function）或是代价函数（cost function）来实现。

设树 $T$ 的叶节点个数为 $|T|$ ,$t$ 是树 $T$ 的叶节点，该叶节点上有 $N_t$ 个样本，其中 $k$ 类的样本有 $N_{tk}$ 个，$k=1,2,...,K$，$H_t(T)$ 为叶节点 $t$ 上的经验熵，$\alpha\geq0$ 为参数，则决策树的损失函数可以定义为：
$$
C_\alpha(T)=\sum_{t=1}^{|T|}N_tH_t(T)+\alpha |T|
$$
其中经验熵为：
$$
H_t(T)=-\sum_k\frac{N_{tk}}{N_t}log\frac{N_{tk}}{N_t}
$$
在损失函数中，上右边第一项可以写为：
$$
C(T)=\sum_{t=1}^{|T|}N_tH_t(T)=-\sum^{|T|}_{t=1}\sum_{k=1}^{K}N_{tk}log\frac{N_{tk}}{N_t}
$$
这时会有：
$$
C_\alpha(T)=C(T)+\alpha|T|
$$
在上面的公式中，$C(T)$ 表示模型对训练数据的预测误差，也就是 **模型与训练数据的拟合程度**，$|T|$ 表示模型的复杂程度，参数 $\alpha\geq0$ 控制二者之间的影响。当 $\alpha=0$ 时表示不考虑模型的复杂度，只考虑模型与训练数据的拟合程度。

而剪枝操作，就是在 $\alpha$ 确定时，选择损失函数最小的模型，也就是损失函数最小的子树。

因此，可以看出，决策树的生成只考虑了通过提高信息增益（或是信息增益比）对训练数据进行更好的拟合。而决策树剪枝通过优化损失函数还考虑了减小模型复杂度。**决策树生成学习局部的模型，而决策树剪枝学习整体的模型**。 



## 回归树的生成

假设 $X,Y$ 分别为输入和输出变量，并且 $Y$ 是连续变量，给定训练数据集：

$$D={(x1,y1),(x2,y2),...,(x_N,y_N)}$$ 

一个回归树对应着输入空间（也就是特征空间）的一个划分以及在划分的单元上的输出值。

假设已将输入空间划分为 $M$ 个单元 $R_1,R_2,...,R_M$，并且在每个单元 $R_m$ 上有一个固定的输出值 $c_m$，于是回归树模型可以表示为：

$$f(x)=\sum_{m=1}^{M}c_mI(x\in R_m)$$

当输入空间确定时，可以用平方误差 $\sum_{x_i\in R_m}(y_i-f(x_i))^2$ 来表示对于训练数据的预测误差，用平方误差最小的准则求解每个单元上的最优输出值。

可以知道

> 单元 $R_m$ 上的 $c_m$ 的最优值 $\hat{c}_m$ 是 $R_m$ 上的所有输入实例 $x_i$ 对应的输出 $y_i$ 的均值，也就是：

$$
\hat{c}_m=ave(y_i|x_i\in R_m)
$$

因此接下来的主要问题就是 **如何对输入空间进行划分**，使用启发式的方法，这里选择第 $j$ 个变量 $x^{(j)}$ 和它的取值$s$，作为 **切分变量（splitting variable）**和 **切分点（splitting point）**，并定义下面两个区域：
$$
R_1(j,s)=\{x^{(j)}\leq s\}\ \ \ \ \ \ 和\ \ \ \ \ \ R_2(j,s)=\{x^{(j)}> s\}
$$
对于固定的切分变量可以找到最优的切分点 $s$，也就是：
$$
min_{j,s}\left[min_{c1}\sum_{x_i\in R_1(j,s)}(y_i-c_1)^2+min_{c2}\sum_{x_i\in R_2(j,s)}(y_i-c_2)^2\right]
$$
其中：
$$
\hat{c}_1=ave(y_i|x_i\in R_1(j,s))\ \ \ \ \ \ 和\ \ \ \ \ \ \hat{c}_2=ave(y_i|x_i\in R_2(j,s))
$$
遍历所有变量，找到最优的切分变量 $j$，构成一对$(j,s)$。并依此将输入空间划分为两个区域。接着，对每个区域重复上述的划分过程，直到满足条件为止。

上面这样的回归树就是 **最小二乘回归树**。

---



## CART 剪枝

CART 剪枝算法就是从“完全生长” 的决策树的底端减去一些子树，使得决策树变小（模型变简单），从而能够对位置数据有更准确的预测。

CART 剪枝算法有两步组成：

> 1. 从生成算法产生的决策树 $T_0$ 的底端开始不断剪枝，知道 $T_0$ 的根节点，形成一个子树序列 $\{T_0,T_1,...,T_N\}$；
> 2. 通过交叉验证法在独立的验证数据集上对子树序列进行测试，从中选择出最优的子树。



具体如下所示：

### 1. 剪枝，形成一个子树序列 

在剪枝的过程中，计算子树的损失函数：
$$
C_{\alpha}(T)=C(T)+\alpha|T|
$$
其中，$T$ 为任意子树，$C(T)$ 是对<u>训练数据</u>的预测误差（如基尼指数），$|T|$ 为子树的叶节点数，$\alpha \geq 0$ 为参数，$C_\alpha(T)$ 为参数是 $\alpha$ 的整体损失。参数 $\alpha$ 用来权衡训练数据的拟合程度与模型的复杂度。

&emsp;&emsp;对于固定的 $\alpha$ ，一定存在使损失函数 $C_\alpha(T)$ 最小的子树，将其表示为 $T_\alpha$，因此可以说 $T_\alpha$ 在损失函数  $C_\alpha(T)$ 的情况下是最优的。

&emsp;&emsp;可以知道：

> - 当 $\alpha$ 偏大的时候，最优子树 $T_\alpha$ 偏小。
> - 当 $\alpha$ 偏小的时候，最优子树 $T_\alpha$ 偏大。
> - 当 $\alpha =0$ 的时候，整体树是最优的；
> - 当 $\alpha \rightarrow  \infty $ 的时候，根节点组成的单节点树是最优的

&emsp;&emsp;可以证明，可以使用递归的方法对树进行剪枝

> 将$\alpha$ 从小增大，$0=\alpha_0<\alpha_1<,...,<\alpha_n<+\infty$，产生一系列的区间，$[\alpha_i,\alpha_{i+1}),i=0,1...,n$ ;
>
> 剪枝得到的子数列对应着区间 $[\alpha_i,\alpha_{i+1}),i=0,1...,n$ 的最优子序列 $\{T_0,T_1,...,T_n\}$，序列中的子树是嵌套的。



对 $T_0$ 中的每一个节点 $t$，计算：
$$
g(t)=\frac{C(t)-C(T_t)}{|T_t|-1}
$$
它表示剪枝后整体损失函数减少的程度。在  $T_0$ 中剪去 $g(t)$ 最小的子树 $T_t$，得到的子树作为 $T_1$，同时将最小的 $g(t)$ 设为 $\alpha_1$，则 $T_1$ 就是区间 $[\alpha_1,\alpha_2)$ 的最优子树

&emsp;&emsp;如此剪枝下去，直到得到根节点。在这个过程中，不断增加 $\alpha$ 的值，产生新的区间。

###  2.在剪枝得到的子树序列 $\{T_0,T_1,...,T_N\}$ 通过交叉验证产生最优的子树

&emsp;&emsp;具体的，利用独立的验证数据集，测试子树序列 $\{T_0,T_1,...,T_N\}$ 中各棵子树的平方误差或基尼指数。平方误差或基尼指数最小的决策树被认定为是最优的决策树。

&emsp;&emsp;因为在子树序列中，每棵子树 $T_0,T_1,...,T_N$ 都对应一个参数 $\alpha_1, \alpha_2,...,\alpha_n$。所以，当最优子树 $T_k$ 选定后，对应的 $\alpha_k$ 也就确定了，也就得到了最优的决策树。



## 缺失值数据的处理

&emsp;&emsp;对于数据缺失，主要面临两个问题：

> 1. 如何在属性值缺失的情况下进行划分属性选择？
> 2. 给定划分属性，若样本在该属性上的值缺失，如何对样本进行划分？



&emsp;&emsp;给定训练数据集 $D$ 和属性 $a$，我们令 $\tilde{D}$ 表示在 $D$ 中在属性 $a$ 上没有缺失值的样本子集。

1. 对于问题（1）：

   显然我们仅可以根据 $\tilde{D}$ 来判断属性 $a$ 的优劣。

   > 假定属性 $a$ 有 $V$ 个可取值 $\{a^1,a^2,...,a^v\}$，令$\tilde{D}^v$ 表示 $\tilde{D}$ 中在属性 $a$ 上取值为 $a^v$ 的样本子集
   >
   > $\tilde{D}^k$ 表示 $\tilde{D}$ 中属于第 $k$ 类$(k=1,2,...,\mathcal{|Y|})$ 的样本子集，则显然有 $\tilde{D}=\cup_{k=1}^{|\mathcal{|Y|}|}\tilde{D}^k,\tilde{D}=\cup_{v=1}^{|\mathcal{|V|}|}\tilde{D}^v$
   >
   > 假定我们为每个样本 $x$ 赋予一个权重 $\omega_x$，并进行定义：
   > $$
   > \rho=\frac{\sum_{x\in \tilde{D}}\omega_x}{\sum_{x\in {D}}\omega_x}
   > $$
   >
   > $$
   > \tilde{p}_k=\frac{\sum_{x\in \tilde{D}_k}\omega_x}{\sum_{x\in \tilde{D}}\omega_x}, (1\leq k\leq|\mathcal{Y}|)
   > $$
   >
   > $$
   > \tilde{r}_v=\frac{\sum_{x\in \tilde{D}^v}\omega_x}{\sum_{x\in \tilde{D}}\omega_x}, (1\leq k\leq|\mathcal{Y}|)
   > $$
   >

   直观的看，对于属性 $a$，$\rho$ 表示无缺失样本所占的比例，$\tilde{p}_k$ 表示无缺失样本中第 $k$ 类所占的比例，$\tilde{r}_v$ 表示无缺失值样本中在属性 $a$ 上取值 $a^v$ 的样本所占的比例。

   基于上述的定义，我们可以将信息增益的计算式推广为：
   $$
   Gain(D,a) =\rho \times Gain(\tilde{D},a)\\\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ =\rho \times \left(Ent(\tilde{D})-\sum_{v=1}^{V}\tilde{r}_vEnt(\tilde{D}^v)\right)
   $$
   其中：

   ​
   $$
   Ent(\tilde{D})=-\sum_{k=1}^{|\mathcal Y|}\tilde{p}_klog_2\tilde{p}_k
   $$

2. 对于问题（2）

   如果样本  $x$ 在划分属性 $a$ 上的取值已知，则将 $x$ 划入与其对应的子节点，且样本权重在子节点中保持为 $\omega_x$

   如果未知，**则将$x$ 同时划入所有子节点，且样本权重在与属性值 $a^v$ 对应的子节点中调整为 $\tilde{r}·\omega_x$**。直观的看，就是让同一个样本以不同的概率划入到不同的子节点中去。





























