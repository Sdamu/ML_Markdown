## 1. 距离计算

&emsp;&emsp;当对一些没有 Label 的数据进行分类的时候，需要满足一些的基本性质：

1. 非负性：$dist(x_i,x_j)\geq 0$；
2. 同一性：$dist(x_i,x_j)=0$，当且仅当 $x_i=x_j$；
3. 对称性：$dist(x_i,x_j)=dist(x_j,x_i)$；
4. 直递性：$dist(x_i,x_j)=dist(x_j,x_k)+dist(x_k,x_j)$；

给定样本 $x_i=(x_{i1};x_{i2};...,x_{in})$ 与 $x_j=(x_{j1};x_{j2};...;x_{jn})$，最常用的是 “闵可夫斯基距离”(Minkowski distance)
$$
dist_{mk}(x_i,x_j)=\left(\sum_{u=1}^n|x_{iu}-x_{ju}|^p\right)^{\frac{1}{p}}
$$
对于  $p\geq1$，显然满足上面的四条距离度量的基本性质。

$p=2$ 时，闵可夫斯基距离转换为欧氏距离（Euclidean distance)
$$
dist_{ed}(x_i,x_j)=||x_i-x_j||_2=\sqrt{\sum_{u=1}^n|x_{iu}-x_{ju}|^2}
$$
$p=1$ 时，闵可夫斯基距离转换为曼哈顿距离（Manhattan distance）
$$
dist_{man}(x_i,x_j)=||x_i-x_j||_1=\sum_{u=1}^n|x_{iu}-x_{ju}|
$$
&emsp;&emsp;我们在划分属性的时候，通常会将属性划分为连续属性和离散属性；但是在进行距离计算的时候，属性上是否定义了 "序" 的概念才是我们关心的，例如定义域为 $\{1,2,3\}$ 的离散属性，能够直接在属性值上计算距离：

> "1" 与 “2” 比较接近，与 “3” 比较远

这样的属性称为 “有序属性”(ordinal attribute)；而定义域为 $\{飞机，火车，轮船\}$ 这样的离散属性则不能直接在属性值上计算距离，这样的属性称为 “无序属性”(non-ordinal attribute)。显然，闵可夫斯基距离适用于有序属性。

&emsp;&emsp;对于无序属性来说，可以采用 $VDM$（Value Difference Meric）.我们令 $m_{u,a}$ 表示在属性 $u$ 上取值为 $a$ 的样本数，$m_{u,a,i}$ 表示在第 $i$ 个样本簇上的属性 $u$ 上，取值为 $a$ 的样本数，$k$ 为样本簇数，则属性 $u$ 上两个离散值 $a$ 与 $b$ 之间的 VDM 距离为：
$$
VDM_p(a,b)=\sum_{i=1}^k\left|\frac{m_{u,a,i}}{m_{u,a}}-\frac{m_{u,b,i}}{m_{u,b}}\right|^p
$$
因此，可以将闵可夫斯基距离与 VDM 距离结合就能够处理混合属性。

假设有 $n_c$ 个有序属性，$n-n_c$ 个无序属性 ，为了不失一般性，我们令有序属性排列在无序属性之前，则有
$$
MinkovDM_p(x_i,x_j)=\left(\sum_{u=1}^{n_c}|x_{iu}-x_{ju}|^p+\sum_{u=n_c+1}^nVDM(x_{iu},x_{ju})\right)^\frac{1}{p}
$$
当样本空间中不同属性的重要性不同时，可以使用 "加权距离"（weighted distance）.以加权闵可夫斯基距离为例：
$$
dist_{wmk}(x_i,x_j)=(w_1·|x_{i1}-x_{j1}|^p+...+w_n·|x_{in}-x_{jn}|^p)^\frac{1}{p}
$$
其中权重 $w_i\geq0\ (i=1,2,...,n)$ 表示不同属性之间的重要性，通常有 $\sum_{i=1}^nw_i=1$。

---



## 2. 原型聚类

&emsp;&emsp;原型聚类也叫做基于原型的聚类，这类算法假设聚类的结构能够通过一组原型进行刻画，其中，"原型" 指的是样本空间中具有代表性的点。通常轻卡u，算法先对原型进行初始化，然后对原型进行迭代更新求解。采用不同的原型标识。不同的求解方式，将产生不同的算法。



### 2.1 k-means 算法

&emsp;&emsp;给定样本集 $D=\{x_1,x_2,...,x_m\}$，$k$ 均值 算法针对聚类所得粗划分 $\mathcal{C}=\{C_1,C_2,...,C_k\}$ 最小化平方误差：
$$
E=\sum_{i=1}^k\sum_{x\in C_i}||x_i-\mu_i||_2^2
$$
其中 $\mu_i=\frac{1}{|C_i|}\sum_{x\in C_i}x$ 是簇 $C_i$ 的均值向量。只管上来看，上式在一定程度上刻画了簇内样本围绕簇均值向量的紧密程度，$E$ 值越小则紧密程度越高。

&emsp;&emsp;想要求出最优解需要考察样本集 $D$ 所有可能的簇划分，这是一个 $NP$ 难问题。因此， $k$ 均值算法采用贪心策略，通过迭代优化来近似求解。首先看一下 k-means 算法的要点

- 对于 k-means ，首先要注意的是 k 值得选择，一般来说，我们会根据对数据的先验经验选择一个合适的 k 值，如果没有什么鲜艳只是，则可以通过交叉验证选择一个合适的 k 值。
- 在确定了 k 的个数后，我们需要选择 k 个初始化的质心，由于是启发式的方法，因此 k 个初始化的质心的位置选择对最后的聚类结果和运行时间都有很大的影响，因此需要选择合适的质心，最好的是这些质心不能太近。

下面介绍 k-means 算法的大致流程

- 输入：样本集 $D=\{x_1,x_2,...,x_m\}$，聚类的簇数，最大迭代次数 N

- 输出：簇划分 $C=\{C_1,C_2,...,C_k\}$

  1. 从数据集 D 中随机选择 k 个样本作为初始的 k 个质心向量：$\mu_1,\mu_2,...,\mu_k$

  2. 对于 $n=1,2,...,N$

     a. 将簇划分 C 初始化为 $C_t=\varnothing,\ t=1,2,...,k$

     b. 对于 $i=1,2,...,m$，计算样本 $x_i$ 和各个质心向量 $\mu_j,j=(1,2,...,k)$ 的距离：$d_{ij}=||x_i-\mu_j||^2$，将 $x_i$ 标记为最小的为 $d_{ij}$ 所对应的类别 $\lambda_i$。此时更新 $C_{\lambda_j}\cup \{x_i\}$。

     c. 对于 $j=1,2,...,k$，对于 $C_j$ 中所有的样本点重新计算新的质心 $\mu_j=\frac{1}{|C_j|}\sum_{x\in C_j}x$

     d. 如果所有的 k 个质心向量都没有发生变化，则转到步骤 3

  3. 输出簇划分 $C=\{C_1,C_2,..,C_k\}$



可以看出，在上面的算法中，k 个质心的位置选择对最后的聚类结果和运行时间都有很大的影响，因此需要选择合适的 k 个质心。如果仅仅是完全随机的选择，有可能导致算法收敛很慢。



#### 2.1.1 k-means ++

&emsp;&emsp;&emsp;k-means++ 算法是对初始化质心的选择进行优化。它的优化策略也很简单，如下所示：

1. 从输入的数据点集合中随机选择一个点作为第一个聚类中心 $\mu_1$；

2. 对于数据集中的每一个点 $x_i$，计算它与已选择的聚类中心中最近聚类中心的距离；
   $$
   D(x)=arg\ min\sum_{r=1}^{k_{selected}}||x_i-\mu_r||^2_2
   $$

3. 选择一个新的数据点作为新的聚类中心，选择的原则是：$D(x)$ 较大的点，被选取作为聚类中心的概率较大

4. 重读 b 和 c 直到选择出 k 个聚类质心

5. 利用这 k 个执行来作为初始化质心去运行标准的 k-means 算法



#### 2.1.2 elkan k-means

&emsp;&emsp;在传统的 k-means 算法中，我们在每轮迭代的时候，要计算所有的样本点到所有的质心的距离。这样会比较的耗时，因此，**elkan k-means** 就是从这个角度入手对原始的算法加以改进，它的目标是减少不必要的距离的计算。

&emsp;&emsp;**elkan k-means** 利用的原理就是两边之和大于等于第三边，以及量表之差小于第三边的三角形的性质，以此来减少计算次数。

1. 对于一个样本点 $x$ 和两个质心 $\mu_{j1},\mu_{j2}$，如果我们预先计算出了这两个质心之间的距离 $D(j_1,j_2)$，则如果计算发现了 $2D(x,j1)\leq D(j_1,j_2)$，则我们立刻就可以知道 $D(x,j_1)\leq D(x,j_2)$。此时我们不需要再计算 $D(x,j_2)$，也就是说省了一步距离计算。
2. 对于一个样本点 $x$ 和两个质心 $\mu_{j1},\mu_{j2}$ 。我们可以得到 $D(x,j_2)\geq max(\{0,D(x,j_1)-D(j_1,j_2))$。

利用上面两个规律，**elkan k-means** 算法比传统的 k-means 算法的迭代速度有很大的提高。但是如果我们的样本是稀疏的，有缺失值的话，这个方法就不适用了，此时某些距离无法计算，则不能使用该算法。



#### 2.1.3Mini batch K-means

&emsp;&emsp;在传统的 k-means 中，要计算所有的样本点到所有的质心的距离。如果样本量非常大，比如说达到了 10w 以上，特征又达到 100 以上，此时使用传统的 k-means 算法将会非常的耗时，就算加上上面的两种策略也并不能很明显的起到相应的效果。因此。对于大数据，**Mini batch k-means** 也就产生了。

&emsp;&emsp;这种算法的主要思想就是使用样本集中的一部分的样本来做传统的 k-means，这样可以避免样本量太大时的计算难题，算法收敛速度回大大的加快。当时相应的代价就是聚类的精度也会有一些降低。一般来说，这个降低幅度在可以接受的范围之内。

&emsp;&emsp;在 **Mini batch k-means** 中，我们会选择一个合适的批次样本大小的 batch size，我们仅仅用 batch size 个样本来做 k-means 聚类，这 batch size 个样本通常是通过无放回的随机采样得到的。

&emsp;&emsp;为了增加算法的准确性，我们一般会多跑几次 **Mini batch k-means** 算法，用得到不同的随机采样集来得到聚类簇，选择其中最优的聚类簇。



#### 2.1.4 k-means 与 KNN



&emsp;&emsp;k-means 是无监督学习的聚类算法，没有样本输出；而 KNN 是监督学习的分类算法，有对应的类别输出。KNN 基本上并不需要进行训练，对测试集里面的点，只要找出在训练集中最近的 K 个点，用这最近的 K 个点的类别来决定测试点的类别。而 k-means 则有明显的训练过程，找到 k 个类别的最佳质心，从而决定样本的簇类别。

&emsp;&emsp;两者的共同点就是都利用了最近邻的思想。



#### 2.1.5 k-means 优缺点

&emsp;&emsp;优点：

1. 算法原理比较简单，实现容易，首先速度快；

2. 聚类效果较好；

3. 算法的可解释性比较强；

4. 主要需要调整的参数只有类别数 k；

   缺点：

- k 值的选取不好把握；
- 对于不是 凸 的数据集比较难收敛；
- 如果各隐含类别的数据不平衡，比如各隐含类别的数据量严重失衡；或者各隐含类别的方差不同，则聚类效果不佳；
- 采用迭代的方法，得到的结果只是 局部最优；
- 对噪音和异常点比较敏感；



### 2.2 学习向量量化  —— LVQ

&emsp;&emsp;与上面的 k-means 算法类似，学习向量量化 也是试图找到一组原型向量来刻画聚类结构，但是和一般聚类算法不同的是，LVQ 假设数据样本带有类别标记，学习过程中利用样本的这些监督信息来进行辅助聚类。

&emsp;&emsp;给定样本集 $D=\{(x_1,y_1),(x_2,y_2),...,(x_m,y_m)\}$ ，每个样本 $x_j$ 是由 n 个属性描述的特征向量 $(x_{j1},x_{j2},...,x_{jn}),y_i\in \mathcal{Y}$，是样本 $x_j$ 的类别标记。LVQ 的目标是学习一组 $n$ 维原型向量 $\{\textbf{p}_1,\textbf{p}_2,...,\textbf{p}_q\}$，每个原型向量代表一个聚类簇，簇标记 $t_i\in \mathcal{Y}$ 。

&emsp;&emsp;具体算法如下所示：
![这里写图片描述](//img-blog.csdn.net/2018031810224779?watermark/2/text/Ly9ibG9nLmNzZG4ubmV0L3FxXzM0Nzg0NzUz/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
如上面算法所示，算法首先对原型向量进行初始化，例如第 q 个簇可以从类别标记为 $t_q$ 的样本中随机选取一个作为原型向量。算法第 2~12 行对原型向量进行迭代优化。在每一轮迭代中，算法随机选取一个有标记训练样本，找出与其距离最近的原型向量，并根据而这的类别标记是否一致来对原型向量进行相应的更新，如果算法的停止条件已经满足，则将当前原型向量作为最终结果进行返回。

&emsp;&emsp;在学的一组原型向量后，就可以实现对样本空间 $\mathcal{X}$ 的簇划分，对于人一样笨 $x$ ,它将会被划入与其距离最近的原型向量所代表的簇中



### 2.3 高斯混合聚类

&emsp;&emsp;高斯混合聚类采用的是概率模型来表达聚类原型。

&emsp;&emsp;首先给出多元高斯分布的定义。对于 n 维样本空间 $\mathcal{X}$ 中的随机向量 $x$，如果 $x$ 服从高斯分布，则其概率密度函数为：
$$
p(x)=\frac{1}{(2\pi)^{\frac{n}{2}}|\Sigma|}e^{-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)}
$$
其中，$\mu$ 是 n 维均指向量，$\Sigma$ 是 $n\times n$ 的协方差矩阵。从上面的公式可以看出，高斯分布完全由均值向量 $\mu$ 和协方差矩阵 $\Sigma$ 这两个参数确定，为了明确显示高斯分布与相应参数的依赖关系，将概率密度函数记为 $p(x|\mu,\Sigma)$。

&emsp;&emsp; 虽然钙素分布有一些重要的分析性质，但是当它遇到实际的数据集时，也会存在巨大的局限性。如以下两图所示：
![这里写图片描述](//img-blog.csdn.net/20180318102319841?watermark/2/text/Ly9ibG9nLmNzZG4ubmV0L3FxXzM0Nzg0NzUz/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

显然，第一幅图是使用单高斯模型进行数据拟合，第二幅图是使用多高斯（混合）模型进行数据拟合，可以看出，第二张图的拟合效果要明显好于第一张图。

&emsp;&emsp;通过将更基本的概率分布（如 高斯分布）进行线性组合的这样的叠加方法，可以被形式化为概率模型，这样的模型被称为 **混合模型**（mixture distributions）。通过使用足够多的高斯分布，并且调节他们的举止和方差以及线性组合的系数，几乎所有的连续概率密度函数都能够以任意的精度进行近似。

&emsp;&emsp;可以定义高斯混合分布：
$$
p_{\mathcal{M}}(x)=\sum_{k=1}^k\alpha_i·p(x|\mu,\Sigma_i)
$$
这个分布一共由 k 个混合成分组成，每个混合成分对应一个高斯分布。其中 $\mu_i,\Sigma_i$ 分别是第 i 个高斯混合成分的参数，而 $\alpha_i>0$ 为相应的混合系数，有 $\sum_{i=1}^k\alpha_i=1$.

&emsp;&emsp;假设样本的生成过程是由高斯混合分布给出的：

1. 根据 $\alpha_1,\alpha_2,...,\alpha_k$ 定义的鲜艳分布选择高斯混合成分，其中 $\alpha_i$ 是选择第 $i$ 个混合成分的概率；
2. 分局被选择的混合成分的概率密度函数进行采样，从而生成相应的样本。



&emsp;&emsp;在给出高斯混合聚类算法之前，首先介绍一个参数 $\gamma_{ji}$，这个参数表示的是样本 $x_j$ 是由第 $i$ 个高斯混合成分生成的后验概率。

&emsp;&emsp;当高斯混合分布已知的情况下，高斯混合聚类将把样本 $D$ 划分为 $k$ 个簇，$\mathcal{C}=\{C_1,C_2,...,C_k\}$，每个样本 $x_j$ 的簇标记 $\lambda_j$ 通过如下表达式进行确定
$$
\lambda_j=arg\ max \ \ \gamma_{ji}
$$


因此，从原型聚类的角度来看，高斯混合聚类是采用概率模型（高斯分布）对原型进行刻画，簇划分则由原型对应的后验概率确定。

&emsp;&emsp;下面给出高斯混合聚类的算法：

![这里写图片描述](//img-blog.csdn.net/20180318102309324?watermark/2/text/Ly9ibG9nLmNzZG4ubmV0L3FxXzM0Nzg0NzUz/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)



可以看出，主要思想是利用 EM 算法进行求得，关于 EM 算法，这里先不展开进行介绍，等到后期会再自己进行介绍推导。



## 3. 密度聚类

&emsp;&emsp;密度聚类也叫做“基于密度的聚类”，这种算法假设聚类结构能通过样本分布的紧密程度来确定。通常情形下，密度聚类算法从样本密度的角度来考察样本之间的可连接性，并基于可连接样本不断扩展聚类簇已获得最终的聚类结果。

&emsp;&emsp;也就是说，通过将紧密向量的样本划为一类，这样就得到了一个聚类类别。通过将所有各组紧密相连的样本划为各个不同的类别，则也就得到了最终的所有聚类类别的结果。



### 3.1 DBSCAN 相关概念

&emsp;&emsp;DBSCAN 是一种著名的密度聚类算法，它是基于一组 ”邻域“ 参数来刻画样本分布的紧密程度。给定数据集 $D=\{x_1,x_2,...,x_m\}$ ，首先定义下面这几个概念：

1. **$\epsilon$-邻域**： 对于 $x_j\in D$，其 $\epsilon$-邻域包含样本集 $D$ 中与 $x_j$ 的距离不大于 $\epsilon$ 的样本，也就是说有 $N_{\epsilon}(x_j)=\{x_i\in D|dist(x_i,x_j)\leq\epsilon\}$ ；（默认为欧氏距离）
2. **核心对象(core object)**：若 $x_j$ 的 $\epsilon$-邻域至少包含 $MinPts$ 个样本，即 $|N_{\epsilon}(x_j)|\geq MinPts$，则 $x_j$ 是一个核心对象；
3. **密度直达**：若 $x_j$ 位于 $x_i$ 的$\epsilon$-邻域中，并且有 $x_i$ 是核心对象，则称 $x_j$ 由 $x_i$ 密度直达。
4. **密度可达**：对 $x_i$ 和 $x_j$，如果存在样本序列 $p_1,p_2,...,p_n$，其中，$p_1=x_i,p_n=x_j$ 且有 $p_{i+1}$ 是由 $p_i$ 密度直达，则称 $x_j$ 由 $x_i$ 密度可达;
5. **密度相连**：对 $x_i$ 和 $x_j$，如果存在 $x_k$，使得 $x_i$ 与 $x_j$ 均由 $x_k$ 密度可达，则称 $x_i$ 与 $x_j$ 是密度相连的。

&emsp;&emsp;基于这些概念，DBSCAN 将 "簇" 定义为：由密度可达关系导出的最大的密度相连样本集合。形式化的说，就是给定邻域参数（$\epsilon,MinPts$），簇 $C\subseteq D$  是满足以下性质的非空样本子集：

- 连接性(connectivity)：$x_i\in C,x_j\in C \Rightarrow x_i$ 与 $x_j$ 密度相连
- 最大性(maximality): $x_i \in C, x_j$ 由 $x_i$ 密度可达 $\Rightarrow x_j\in C$

因此，问题也就变成了如何才能从数据集 D 中找出满足上面性质的聚类簇，事实上，如果 $x$ 为核心对象，由 $x$ 密度可达的所有样本组成的集合记为 $X=\{x'\in D|x' 由 x 密度可达\}$，则不难证明 $X$ 也就是满足连接性与最大性的簇。



### 3.2 DBSCAN 聚类思想

&emsp;&emsp; DBSCAN 聚类定义：由密度可达关系推导出的最大密度相连的样本集合，也就是我们最终聚类的一个类别，或者或是一个 "簇"。

&emsp;&emsp;这个 DBSCAN 的簇里面可以有一个或者多个核心对象。如果只有一个核心对象，则簇里面其他的非核心对象样本都在这个核心对象的 $\epsilon$-邻域内；如果有多个核心对象，则簇里的任意一个核心对象的 $\epsilon$-邻域中一定有一个其他的核心对象，否则这两个核心对象无法密度可达。这些核心对象的 $\epsilon$-邻域邻域里所有的样本的集合组成的一个 DBSCAN 聚类簇。

&emsp;&emsp;因此主要问题就是找到这样的簇样本，DBSCAN 的思想是

> 任意选择一个没有类别的核心对象作为种子，然后找到所有这个核心对象能够密度可达的样本集合，也就是一个聚类簇。接着继续选择另一个没有类别的核心对象去寻找密度可达的样本集合，这样就得到了另一个聚类簇。已知运行到所有核心对象都有类别为止。

&emsp;&emsp;但是还有三个问题没有考虑：

1. 一些异常样本点或者说少量**游离于簇外的样本点**，这些点不在任何一个核心对象周围，在 DBSCAN 中，一般是将这些样本点标记为 **噪音点**。
2. **距离度量的问题**：也就是如何计算某样本和核心对象样本的距离。在 DBSCAN 中，一般采用最近邻思想，采用某一种距离度量来衡量样本距离，比如说欧氏距离。这与 KNN 的思想完全相同。对应少量的样本，寻找最近邻可以直接去计算所有样本的距离，如果样本比集哦啊大，则一般采用 KD树 或者 球树 来快速搜索最近邻。
3. 某些样本可能到两个核心对象的距离都小于 $\epsilon$，但是这两个核心对象由于不是密度直达，又不属于同一个聚类簇，那么这个样本的类别一般用 **先来后到** 的策略，先进行聚类的类别簇会标记这个样本为它的类别。也就是说 DBSCAN 的算法不是完全稳定的算法。

### 3.3 DBSCAN 算法

- 输入：样本集 $D=\{x_1,x_2,...,x_m\}$，邻域参数 $(\epsilon,MinPts)$，样本距离度量方式
- 输出：簇划分 $C$
- 步骤：
  1. 初始化核心对象集合 $\Omega=\varnothing$，初始化聚类簇数 k=0，初始化未访问样本集合 $\mathcal{T}=D$，簇划分$C=\varnothing$
  2. 对于  $j=1,2,...,m$，按照下面的步骤找出所有的核心对象：
     - 通过距离度量方式，找到样本 $x_j$ 的 $\epsilon$-邻域子样本 $N_{\epsilon}(x_j)$
     - 如果子样本集样本个数满足 $|N_{\epsilon}(x_j)|\geq MinPts$，将样本 $x_j$ 加入核心对象的样本集合：$\Omega=\Omega \cup\{x_j\}$
  3. 如果核心对象集合 $\Omega=\varnothing$，则算法结束，否则转入步骤4.
  4. 在核心对象集合 $\Omega$ 中，随机选择一个核心对象 $o$，初始化当前簇核心对象序列 $\Omega_{cur}=\{o\}$，初始化列别序列号 $k=k+1$，初始化当前簇样本集合 $C_k=\varnothing$，更新未访问集合 $\mathcal{T}=\mathcal{T}-\{o\}$
  5. 如果当前簇核心对象队列 $\Omega_{cur}=\varnothing$，则当前聚类簇 $C_k$ 生成完毕，更新簇划分 $C=\{C_1,C_2,..,C_k\}$，更新核心对象集合 $\Omega=\Omega-C_k$，转入步骤3
  6. 在当前簇核心对象队列 $\Omega_{cur}$ 中取出一个核心对象 $o'$，通过邻域距离阈值 $\epsilon$ 找出所有的$\epsilon$-邻域子样本集 $N_{\epsilon}(o')$，令$\Delta=N_{\epsilon}(o')\cap \mathcal{T}$,更新当前簇样本集合 $C_k=C_k\cup\Delta$，更新未访问样本集合 $\mathcal{T}=\mathcal{T}-\Delta$，哥更新 $\Omega_{cur}=\Omega_{cur}\cup (N_{\epsilon}(o')\cap\Omega)$，转入步骤 5

### 3.4 DBSCAN 优缺点

&emsp;&emsp;和传统的 k-means 相比， DBSCAN 最大的不同就是不需要输入类别数 k，它的最大的优势是可以发现任意形状的聚类簇，而不是像 k-means，一般仅仅只能适用于凸的样本集聚类。同时它在聚类的同时还可以找出异常点，这点和 BIRCH 算法类似。

&emsp;&emsp;因此，一般来说，如果数据是稠密的，并且数据集不是凸的，那么用 DBSCAN 会比 k-means 的效果好很多，但是，如果数据集不是稠密的，则不推荐使用 DBSCAN 进行聚类。

- 优点：
  1. 可以对任意形状的稠密数据集进行聚类，相对的，k-means 之类的聚类算法一般只适用于凸数据集；
  2. 可以在聚类的同时发现异常点，对数据集中的异常点不敏感；
  3. 聚类结果没有偏倚，相对的 k-means 之类的聚类算法初始值对聚类结果有很大影响;
- 缺点：
  1. 如果样本集的密度不均匀、聚类间距相差很大时，聚类质量较差，这时用 DBSCAN 聚类一般不适合
  2. 如果样本集比较大时，聚类收敛时间较长，此时可以对搜索最近邻时建立的 KD树 或者球树 进行规模限制来改进
  3. 调参相对于传统的 k-means 之类的聚类算法稍复杂，主要需要对距离阈值 $\epsilon$ ，邻域样本数阈值 $MinPts$ 联合调参，不同的参数组合对最后的聚类效果有较大影响。



## 4. 层次聚类

&emsp;&emsp;层次聚类（hierarchical clustering) 试图在不同层次上对数据集进行划分，从而形成树形的聚类结构。数据集的划分可以采用 “自底向上” 的聚合策略，也可以采用 “自顶向下” 的分拆策略。



### 4.1 AGNES 相关概念

&emsp;&emsp;AGNES 算法是一种采用自底向上的聚合策略的层次聚类算法。

1. 首先将数据集中的每个样本看作是一个厨师聚类簇
2. 然后在算法运行的每一步中找出距离最近的两个聚类簇进行合并，这个过程不断进行重复，直到达到预设的聚类簇个数。

上面这里的关键是 如何计算聚类簇之间的距离。实际上，每一个聚类簇是一个样本集合，因此，只需要采用关于集合的某种距离即可。



&emsp;&emsp;例如，给定聚类簇 $C_i$ 与 $C_j$，可以通过下面的公式来计算集合之间的距离：
$$
最小距离：d_{min}=(C_i,C_j)=min_{x\in C_i,z\in C_j}dist(x,z)\\
最大距离：d_{max}=(C_i,C_j)=max_{x\in C_i,z\in C_j}dist(x,z)\\
平均距离：d_{avg}=(C_i,C_j)=\frac{1}{|C_i||C_j|}\sum_{x\in C_i,z\in C_j}dist(x.z)
$$
&emsp;&emsp;很显然，最小距离是由两个簇的最近样本决定的，最大距离由两个簇的最远样本决定，而平均距离则有两个簇的所有样本共同决定。当聚类簇距离由 $d_{min}、d_{max}、d_{avg}$ 计算时， AGNNES 算法被相应地称为 ”单链接(single-linkage)“、"全链接(complete-linkage)" 或 “均连接(average-linkage)”。



### 4.2 AGNES 算法
![这里写图片描述](//img-blog.csdn.net/20180318102441532?watermark/2/text/Ly9ibG9nLmNzZG4ubmV0L3FxXzM0Nzg0NzUz/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
&emsp;&emsp;上图中显示的也就是 AGNES 算法的具体流程，可以看到，算法的前 9 行首先对仅含有一个样本的初始聚类簇和相应的距离矩阵进行初始化；然后， AGNES 不断合并举例最近的聚类簇，并对合并得到的聚类簇的距离矩阵不断进行更新；上述过程不断重读重复，直到达到预设的聚类簇数。

---

## 6. 参考
1. 周志华——机器学习
2. PRML
3. [聚类](http://www.cnblogs.com/pinard/p/6221564.html)