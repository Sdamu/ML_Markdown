# L1 与 L2 正则化

##1. 通常基本理解

&emsp;&emsp;L1 正则化和 L2 正则化可以看做是损失函数的惩罚项。 所谓的惩罚就是就损失函数中的某些参数做一些限制。对于线性回归模型，使用 L1 正则化的模型叫做 Lasson 回归，使用 L2 正则化的模型叫做 Ridge 回归（岭回归）。下面两个公式分别表示加了 L1 和 L2 正则化的线性回归：
$$
min_{w}\frac{1}{2n_{samples}}||Xw-y||_2^2+\alpha||w||_1\\
min_{w}\frac{1}{2n_{samples}}||Xw-y||_2^2+\alpha||w||_2^2\\
$$
一般的回归分析中回归 $w$ 表示的是特征的系数，从上面两个公式中可以看出正则化项是对系数做了处理（限制范围）。其中：

- L1 正则化项是权值向量 $w$ 中的各个元素的 **绝对值之和**，一般表示为 $||w||_1$
- L2 正则化项是全职向量 $w$ 中各个元素的 **2-范数**，一般表示为 $||w||_2^2$

而正则化项前面的系数表示的是对 $w$ 惩罚的程度，可以看出，它起到一个权衡的作用。

&emsp;&emsp;而 L1 与 L2 的作用分别如下：

- L1 正则化可以产生稀疏权值矩阵，也就是可以产生一个系数模型，可以用于特征选择；
- L2 正则化可以防止模型的过拟合（overfitting）；当然，在一定程度上，L1 正则化可以防止过拟合。

### 1.1 L1 的过滤系数效果

&emsp;&emsp;假设有下面的带 L1 正则化的损失函数：
$$
J=J_0+\alpha\sum|w_i|
$$
其中 $J_0$ 是原始的损失函数，后面的一项表示 L1 正则化，$\alpha$ 是正则化系数。注意到 L1 正则化是权值系数的绝对值之和，也就是说，$J$ 是一个带有绝对值符号的函数，因此 $J$ 是不完全可微的。而机器学习的任务就是要通过一些优化算法求出这个损失函数的最小值（L1 考虑坐标轴下降法）。当我们在原始损失函数 $J_0$ 后加上正则化项时，相当于加上了一个约束，我们令 $L=\alpha\sum|w|_i$，因此，原问题就可以看做是在 **约束 L 的条件下求出 $J_0$ 的最小值**。这里可以考虑最简单的二维的情况，也就是只有两个权值 $w_1,w_2$，此时 $L_1=|w_1|+|w_2|$，此时如果采用梯度下降法，可以将求解 $J_0$ 的过程画作等值线，同事 L1 正则化的函数也可以在 $w_1,w_2$ 的平面上画出来。如下图所示：

![img](https://upload-images.jianshu.io/upload_images/311426-cf1508403b21a4a4?imageMogr2/auto-orient/strip%7CimageView2/2/w/318)

&emsp;&emsp;可以看出，上图中彩色的等值线就是我们要求解的原问题 $J_0$，当 $J_0$ 的等值线与 L 的图像去西安首次相交时就得到了最优解。可以发现上图中是在一个顶点进行相交，因此这个顶点也就是最优解。需要注意的是，这个顶点的坐标是 $(0,w_2)$，可以直观的想象，因为 L 函数有许多 “突出的角点”，二维情况下是 4 个，三维情况下是 8个，如下图所示：

![这里写图片描述](//img-blog.csdn.net/20180320171613456?watermark/2/text/Ly9ibG9nLmNzZG4ubmV0L3FxXzM0Nzg0NzUz/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

而在多维的情况下会更多，那么 $J_0$ 与这些角接触的几率会远远大于与 L 其他部位接触的几率，而在这些角上，会有很多权值都等于 0，这就是为什么 L1 正则化可以产生稀疏的模型，进而可以用于进行特征选择。

&emsp;&emsp;而正则化前面的系数 $\alpha$，可以用来控制 L 的图形的大小，$\alpha$ 越小， L 的图形也就越大，也就是说明这个惩罚项对解空间的约束也就越小，反之则亦然。

&emsp;&emsp;作为对比，假设有如下带有 L2 正则化的损失函数：
$$
J=J_0+\alpha\sum|w_i|^2
$$
同样的，也可以画出在二维平面上的图形，

![这里写图片描述](//img-blog.csdn.net/201803201716288?watermark/2/text/Ly9ibG9nLmNzZG4ubmV0L3FxXzM0Nzg0NzUz/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

可以看出在二维平面下 L2 正则化的函数时一个圆，与上面的方形相比，被“磨去了棱角”。因此当 $J_0$ 与 L 相交的时候，使得 $w_1,w_2$ 正则化等于 0 的概率减小了很多，这就是为什么 L2 正则化不具有稀疏性的原因。

### 1.2 L2 正则化减轻过拟合

&emsp;&emsp;拟合的过程中通常都倾向于使权值尽可能的小，最后构造一个所有参数都比较小的模型。因为一般认为参数值小的模型比较简单，能适应不同的数据集，也在一定程度上避免了过拟合的现象。可以设想一下对于一个线性回归方程，如果参数很大，那么只要数据偏移一点点，那么对于最后的结果来说，就会造成很大的影响。但是如果参数足够小，数据偏移得多一点也不会对结果造成太大的影响。也就是说，小的参数对于数据的 “抗扰动能力强”。

&emsp;&emsp;越是复杂的模型，越是会尝试对所有的样本进行拟合，甚至包括一些异常样本点，这就很容易造成在较小的区间里预测值产生较大的波动，这种较大的波动也反映了在这个区间里的导数很大，而只有较大的参数值才能产生较大的导数。因此复杂的模型，其参数值比较大 。

&emsp;&emsp;因此现在的问题也就变成了 **为什么 L2 正则化可以获得很小的参数**？
$$
L'(\theta)=L(\theta)+\lambda\frac{1}{2}||\theta||_2\\
Gradient:\frac{\partial L'}{\partial w}=\frac{\partial L}{\partial w}+\lambda w\\
Update:\ \ \ w^{t+1}\rightarrow w^t-\eta\frac{\partial L'}{\partial w}=w^t-\eta\left[\frac{\partial L}{\partial w}+\lambda w^t\right]\\
=(1-\eta\lambda)w^t-\eta\frac{\partial L}{\partial w}
$$


从上面的梯度下降公式可以看到，L2 正则化的损失函数在参数更新时，参数是按照固定的比例减少的，也就是说，如果  很大，则每次更新较大的值；假如  较小，则每次更新会减少较小的值，因此，L2 正则化曲线比较平滑。因此在每次迭代中，乘以这个小于 1 的因子后，参数就会不断的减小，从而总的来看，参数是不断减小的。

&emsp;&emsp;这里我们以线性回归中的梯度下降法为例，在梯度下降的过程中，与未添加 L2 正则化的迭代公式相比，每一次迭代，都要先乘以一个小于1 的因此，从而使得参数不断减小，因此总的来看，参数是不断减小的。
$$
L'(\theta)=L(\theta)+\lambda\frac{1}{2}||\theta||_2\\
Gradient:\frac{\partial{L'}}{\partial w}=\frac{\partial L}{\partial w}+\lambda w\\
update:w^{t+1}\rightarrow w^t-\eta\frac{\partial L'}{\partial x}=w^t-\eta\left[\frac{\partial L}{\partial x}+\lambda w^t\right]\\
=(1-\eta\lambda)w^t-\eta\frac{\partial L}{\partial w}
$$
从上面的梯度下降公式可以看到，L2 正则化的损失函数在参数更新时，参数是按照固定的比例减少的，也就是说，如果 $w$ 很大，则每次更新较大的值；假如 $w$ 较小，则每次更新会减少较小的

&emsp;&emsp;当然，L1 正则化在一定程度上也可以防止过拟合。因为当 L1 正则化系数很小的时候，得到的最优解会很小，可以达到和 L2 正则化类似的效果。
$$
L'(\theta)=L(\theta)+\lambda\frac{1}{2}||\theta||_1 \\
Gradient:\frac{\partial L'}{\partial w}=\frac{\partial L}{\partial w}+\lambda sgn(w)\\
Update:\ \ \ w^{t+1}\rightarrow w^t-\eta\frac{\partial L'}{\partial w}=w^t-\eta\left[\frac{\partial L}{\partial w}+\lambda sgn(w^t)\right]\\
=w^t-\eta\frac{\partial L}{\partial w}-\eta\lambda sgn(w^t)
$$
从上面公式可以看出，参数是按照固定的量减少的，如果 $w$ 是正的，则减少一个固定量，如果 $w$ 是负的，则加上一个固定量。因此，如果 $w$ 本来比较小的话，则很容易被更新到 0。

## 2. 正则化与贝叶斯

&emsp;&emsp;首先给出结论：

> 正则化参数等价于对参数引入 **先验分布**，使得 **模型复杂度** 变小（缩小解空间）。其中 L1 正则化相当于引入参数分布符合 拉普拉斯分布，而 L2 正则化相当于引入参数分布符合 高斯分布，对于噪声以及 outliers 的鲁棒性增强（泛化能力）。整个最优化问题从贝叶斯观点来看是一种贝叶斯最大后验估计，其中**正则化项** 对应后验估计中的 **先验信息** ，损失函数对应后验估计中的似然函数，两者的乘积也就对应了贝叶斯最大后验估计的形式。



&emsp;&emsp;当你想从一个手头的数据集中学习出一套规则时，贝叶斯学派认为仅仅使用这些数据是不够的，还需要加入先验知识。如果你在损失函数中使用了L1正则项，那么其实质就是加入了拉普拉斯先验分布，即认为数据是符合拉普拉斯分布的；如果你使用了L2正则项，那么就是加入了高斯先验分布，即认为数据是符合高斯分布的。一般由于推导和计算方便，会对分布函数取对数，然后再去优化。最终的结果是，由于你的模型参数考虑了数据先验，学习出来的规则就更加接近实际。

&emsp;&emsp;我们如果对拉普拉斯密度函数取对数,剩下的是一个一次项|x-u|，这就是L1范式；我们如果对高斯密度函数取对数剩下的就是一个二次项（x-u）^2，这就是L2范式。比较高斯分布的密度函数图像和拉普拉斯分布的密度函数图像，我们很容易看到，当x趋于正无穷和负无穷时，前者是逼近于0的，后者是等于0的。



---

参考：

1. [为什么L1稀疏L2平滑？](http://blog.csdn.net/li8zi8fa/article/details/77649973)
2. [机器学习中正则化项L1和L2的直观理解](https://www.jianshu.com/p/201d9917c578?utm_campaign=maleskine&utm_content=note&utm_medium=seo_notes&utm_source=recommendation)
3. PRML











