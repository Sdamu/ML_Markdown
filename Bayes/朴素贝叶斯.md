# 朴素贝叶斯

&emsp;&emsp;朴素贝叶斯法是基于贝叶斯定理与特征条件独立假设的分类方法。

对于给定的训练数据集，

1. 首先给予特征条件独立假设学习出输入输出的联合概率分布；
2. 然后基于此模型，对给定的输入 $x$，利用贝叶斯定理求出后验概率最大的输出 $y$。

朴素贝叶斯方法实现简单，学习与预测的效率都比较高，是一种常用的方法。



## 基本方法

&emsp;&emsp;设输入空间 $\mathcal{X}\subseteq \textbf{R}^n$ 是 $n$ 维向量的集合，输出空间为类标记集合 $\mathcal{Y}=\{c_1,c_2,...,c_K\}$，输入为特征向量 $x\in \mathcal{X}$，输出为类标记 $y_i\in \mathcal{Y}$。已知 $X$ 是定义在 $\mathcal{X}$ 上的随机向量， Y 是定义在输出空间 $\mathcal{Y}$ 上的随机变量。$P(X,Y)$ 是 $X$ 和 $Y$ 的联合概率分布。训练数据集：
$$
T=\{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\}
$$
是由 $P(X,Y)$ 独立同分布产生。

&emsp;&emsp;朴素贝叶斯通过训练数据集学习联合概率分布 $P(X,Y)$。具体的，学习一下的先验概率分布以及条件概率分布。

- 先验概率分布：
  $$
  P(Y=c_k)\ \ k=0,1,...,K
  $$








- 条件概率分布：
  $$
  P(X=x|Y=c_k)=P(X^{(1)}=x^{(1)},X^{(2)}=x^{(2)},...,X^{(n)}=x^{n)}|Y=c_k)
  $$
  于是可以学习到联合概率分布 $P(X,Y)$。

但是，条件概率分布 $P(X=x|Y=c_k)$ 有指数量级的参数，其估计实际是不可行的。因此，朴素贝叶斯在这里对条件概率做了 **条件独立性假设**，具体如下：
$$
P(X=x|Y=c_k)=P(X^{(1)}=x^{(1)},X^{(2)}=x^{(2)},...,X^{(n)}=x^{n)}|Y=c_k)\\
=\prod_{j=1}^nP(X^{(j)}=x^{j}|Y=c_k)\ \ \ \ \ \ \ \ \ \ \ \ \
$$
上面这个假设是一个较强的假设，朴素贝叶斯法的 “朴素” 也因此得来。、

&emsp;&emsp;朴素贝叶斯方法学习到的是生成数据的机制，因此属于生成模型。上面的条件独立假设实际上是说用于分类的特征在类确定的条件下都是条件独立的。

&emsp;&emsp;利用朴素贝叶斯方法进行分类的时候，对于给定的输入 $x$，通过学习到的模型计算后验概率分布 $P(Y=c_k|X=x)$，然后将后验概率最大的类作为 $x$ 的类进行输出。而后验概率的计算根据贝叶斯定理计算：
$$
P(Y=c_k|X=x)=\frac{P(X=x|Y=c_k)P(Y=c_k)}{\sum_kP(X=x|Y=c_k)P(Y=c_k)}
$$
将条件独立性假设带入上面的式子中，得到：
$$
P(Y=c_k|X=x)=\frac{P(Y=c_k)\prod_{j}P(X^{(j)}=x^{j}|Y=c_k)}{\sum_kP(Y=c_k)\prod_{j}P(X^{(j)}=x^{j}|Y=c_k)}\ \ k=1,2,...,K
$$
这个式子是朴素贝叶斯方法进行分类的基本公式。因此，朴素贝叶斯分类器可以表示为：
$$
y=f(x)=arg\ max_{c_k}\ \frac{P(Y=c_k)\prod_{j}P(X^{(j)}=x^{j}|Y=c_k)}{\sum_kP(Y=c_k)\prod_{j}P(X^{(j)}=x^{j}|Y=c_k)}
$$
观察上式可以看到，分母对于所有的 $c_k$ 都是相同的。因此：
$$
y=arg\ max_{c_k}P(Y=c_k)\prod_jP(X^{(j)}=x^{(j)}|Y=c^k)
$$

## 贝叶斯学习与分类算法

&emsp;&emsp;下面给出朴素贝叶斯法的学习与分类算法：

- 输入：训练书记 $T=\{(x_1,y_1),(x_2,y_2),…,(x_N,y_N)\}$，其中 $x_i=\{x_i^{(1)},x_i^{(2)},…,x_i^{(N)}\}^T$，$x_i^{(j)}$ 是第 $i$ 个样本的第 $j$ 个特征，$x_i^{(j)}\in\{a_{j1},a_{j2},…,a_{jS_j}\}$，$a_{jl}$ 是第 $j$ 个特征可能取得第 $l$ 个值，$j=1,2,…,n,l=1,2,…,S_j,\ \ \ y_i\in\{c_1,c_2,…,c_K\}$；实例 $x$；
- 输出：实例 $x$ 的分类
- 1. 计算先验概率以及条件概率
     $$
     P(Y=c_k)=\frac{\sum_{i=1}^NI(y_i=c_k)}{N},\ \ \ k=1,2,...,K\\
     P(X^{(j)}=a_{jl}|Y=c_k)=\frac{\sum_{i=1}^{N}I(x_i^{(i)}=a_{jl},y_i=c_k)}{\sum_{i=1}^NI(y_i=c_k)}\\
     j=1,2,...,n;\ \ \ l=1,2,...,S_j,\ \ \ k=1,2,...,K
     $$

  2. 对于给定的实例 $x=(x^{(1)},x^{(2)},...,x^{(n)})^T$，计算：
     $$
     P(Y=c_k)\prod_{j=1}^nP(X^{j}=x^{(j)}|Y=c_k),\ \ \ k=1,2,...,K
     $$

- 3. 确定实例 $x$ 的分类
     $$
     y=argmax_{c_k}\ P(Y=c_k)\prod_{j=1}^nP(X^{(j)}=x^{(j)}|Y=c_k)
     $$





## 拉普拉斯平滑

&emsp;&emsp;使用极大似然估计可能会出现所要估计的概率值为 0 的情况，这时会影响到后延概率的计算结果，使分类产生偏差。解决这个问题的方法是采用贝叶斯估计，条件概率的贝叶斯估计就是：
$$
P_{\lambda}(X^{(j)}=a_{jl}|Y=c_k)=\frac{\sum_{i=1}^NI(x^{j}=a_{jl},y_i=c_k)+\lambda}{\sum_{i=1}^{N}I(y_i=c_k)+S_j\lambda}
$$
在上面的公式中，参数 $\lambda$ 常取 1，此时也就是所谓的 **拉普拉斯平滑**（Laplace smoothing）。此时，先验概率的贝叶斯估计是：
$$
P_\lambda(Y=c_k)=\frac{\sum_{i=1}^NI(y_i=c_k)+\lambda}{B+K\lambda}
$$


## 贝叶斯网

&emsp;&emsp;贝叶斯网（Bayesian network) 亦称为”信念网“（belief network)，它借助有向无环图（Directed Acyclic Graph， DAG) 来刻画属性之间的依赖关系，并使用**条件概率表**（Conditional Probability Tabel，CPT）来描述属性集合的联合概率分布。

&emsp;&emsp;为了简化讨论，这里只考虑所有属性都是离散星的，对于连续的属性，条件概率表可以推广位条件概率密度函数。

&emsp;&emsp;具体来说，一个贝叶斯网 B 由结构 G 和参数 $\Theta$ 两部分构成，也就是说 $B=<G,\Theta>$。其中，网络结构 $G$ 是一个有向无环图，其中每个节点对应于一个属性，若两个属性有直接的依赖关系，则他们由一条边连接起来；参数 $\Theta$ 用来定量描述这种依赖关系，假设属性 $x_i$ 在 $G$ 中的父节点集为 $\pi_i$，则 $\Theta$ 包含了每个属性的条件概率表：
$$
\theta_{x_i|\pi_i}=P_B(x_i|\pi_i)
$$

### 结构

&emsp;&emsp;贝叶斯网结构有效地表达了属性间的条件独立性。给定父节点集，贝叶斯网假设每个属性与它的非后裔属性独立，于是 $B=<G,\Theta>$ 将属性 $x_1,x_2,...,x_d$ 的联合概率分布定义为：
$$
P_B(x_1,x_2,...,x_d)=\prod_{i=1}^dP_B(x_i|\pi_i)=\prod_{i=1}^d\theta_{x_i|\pi_i}
$$
&emsp;&emsp;下图中给出的是贝叶斯网络中三个变量之间典型的依赖关系：

![QQ截图20180312105319](C:\Users\Damu\Desktop\QQ截图20180312105319.png)

在 “同父结构” 中，给定父节点 $x_1$ 的取值，则 $x_3$ 与 $x_4$ 条件独立。在 “顺序”结构中，给定 $x$ 的值，则 $y$ 与 $z$ 条件独立。V 型结构也叫做 “冲撞” 结构，给定子节点 $x_4$ 的取值，$x_1，x_2$ 一定不独立；但是，在 $x_4 $ 结构完全未知的前提下，则 V 型结构下 $x_1$ 与 $x_2$ 却是相互独立的。且这样的独立性称为 “边际独立性”。