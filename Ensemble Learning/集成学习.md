 # 集成学习

&emsp;&emsp; 集成学习（ensemble learning）通过构建并结合多个学习起来完成 学习任务，有时也被称为多分类器系统（multi-classifer system)、基于委员会的学习等等。

&emsp;&emsp;通过不同的方式来将多个分类器结合成为一个强分类器，具体可以分为“同质集成”和“异质集成”。其中，同质指的是最后构建的分类器系统仅仅由一种相同的 基分类器构成，而 异质 指的是最后构建的分类器系统由不仅一种基分类器构成，这时候基分类器一般也不叫基分类器，而是“组件学习器”。

&emsp;&emsp;需要说明的是，将一系列的若学习器结合在一起构成想要构成一个更强的学习器（分类器）也是有要求的，也就是说，想要获得好的集成，应当：

- 个体学习器应该有一定的 “准确性”
- 个体学习器之间应该有一定的 ”差异“

总的来说，就是要个体分类器之间做到 ”**好而不同**“。

&emsp;&emsp;但是，所有用来集成的个体学习器都是为了解决同一个问题训练得来的，因此它们显然不可能相互独立，事实上，个体学习器之间的 “准确性” 与 “多样性” 本身就存在着冲突。一般的，在准确性很高之后，想要增加多样性，就要牺牲一定的 准确性。



&emsp;&emsp;根据个体学习器的生成方式，目前的集成学习方法大致可以分成两大类：

1. 个体学习器之间存在前依赖关系，必须穿串行生成的序列化方法
2. 个体学习器之间不存在强依赖关系、可同时生成的并行化方法；

第一种的代表方法是 “boosting”，后面一中的代表方法是 “Bagging” 和 "随机森林"。



## 结合策略

&emsp;&emsp;这部分参考[集成学习原理小结](http://www.cnblogs.com/pinard/p/6131423.html)

&emsp;&emsp;假设用来进行集成学习的几个分类器分别为 $\{h_1,h_2,...,h_m\}$

1. 平均结合法：

   &emsp;&emsp;对于数值类的回归问题，通常使用的结合策略是 平均法，也就是说，对于若干和若学习器的输出进行平均得到最终的预测输出。

   - 算术平均，即为：
     $$
     H(x)=\frac{1}{T}\sum_1^Th_i(x)
     $$

   - 加权结合，假设每个学习器有一个权重 $w$

   $$
   H(x)=\sum_{i=1}^Tw_ih_i(x)
   $$

   &emsp;&emsp;其中 $w_i$ 是个体学习器 $h_i$ 的权重，通常有：
   $$
   w_i\geq0,\ \ \ \sum_{i=1}^Tw_i=1
   $$

2. 投票法：

   &emsp;&emsp;对于分类问题的预测，我们通常使用的是投票法，假设我们的预测类别是 $\{c_1,c_2,...,c_k\}$，对于任意一个预测样本 $x$，我们的 $T$ 个若学习器的预测结果是 $\{h_1(x),h_2(x),...,h_T(x)\}$。

   &emsp;&emsp;a. 相对多数投票法，也就是我们常说的少数服从多数。如果最后不止一个类别获得了最高票数，则随机选择一个做最终的类别。

   &emsp;&emsp;b. 绝对多数投票法，也就是我们常说的票数必须要过半。在上面相对多数投票法的基础上，不光要求获得最高票数，同时要求票数必须过半，否则学习器会拒绝预测。

   &emsp;&emsp;c. 加权投票法，与健全平均法一样，每个若学习器的分类票数要乘以一个权重，最终将各个来别的加权票数进行求和，最大的值对应的类别为最重类别。

3. 学习法：

   &emsp;&emsp;上面的方法都是对弱学习器的结果做平均或者投票，相对比较简单，但是可能学习误差较大，于是就有了学习法这种方法，对于学习法，代表方法是stacking，当使用stacking的结合策略时， 我们不是对弱学习器的结果做简单的逻辑处理，而是 **再加上一层学习器**，也就是说，我们**将训练集弱学习器的学习结果作为输入，将训练集的输出作为输出，重新训练一个学习器来得到最终结果**。

   &emsp;&emsp;在这种情况下，我们将弱学习器称为初级学习器，将用于结合的学习器称为次级学习器。对于测试集，我们首先用初级学习器预测一次，得到次级学习器的输入样本，再用次级学习器预测一次，得到最终的预测结果。





## Boosting

&emsp;&emsp;Boosting 算法是一族可以将若学习器提升为强学习器的算法。工作机制大致如下：

> - 首先从出事训练集训练出一个 及学习期。
> - 在根据基学习器的表现对训练样本进行调整，使得新签基学习器做错的训练样本在后续受到更多的关注
> - 基于调整后的样本分布来训练下一个基学习器
> - 如此冲进行，直到基学习器数目达到了事先指定的数目 $T$，最终将这 $T$ 个基学习器进行加权结合

上面的步骤中最后一个写到了 “进行加权结合”，这种加权结合是一种 平均结合方法。关于结合方法在上面已经进行了相关的介绍。



#### Adaboost

&emsp;&emsp; 对于所有的 Boosting 算法来说，都有两个问题需要解答：

1. 在每一轮如何改变训练数据的权值或概率分布？
2. 如何将弱分类器组合成为一个强分类器？

关于问题一，Adaboost 的做法是：

> 提高那些前一轮被错误分类样本的权值，而降低那些前一轮被正确分类的样本的权值。

关于问题二，Adaboost 的做法是：

> 采用加权多数表决的方法。也就是说，加大分类误差率小的弱分类器的权值，使其在表决中起较大的作用，减小分类误差率大的弱分类器的权值，使其在表决中起比较小的作用。  



##### Adaboost 算法

&emsp;&emsp;假设给定一个二分类的训练数据集：
$$
T=\{(x_1,y_1),(x_2,y_2)m,...,(x_N,y_N)\}
$$
其中，每个样本点由实例与标记组成。其中，实例 $x\in\mathcal{X}\subseteq \textbf{R}^n$，标记 $y\in\mathcal{Y}=\{-1,+1\}$，其中 $\mathcal{X}$ 是实例空间， $\mathcal{Y}$ 是标记集合。Adaboost 使用下面的算法，从训练数据中学习出一系列弱分类器或是基本分类器，并将这些弱分类器 **线性组合** 成为一个强分类器。

- 输入：训练数据集 $T=\{(x_1,y_1),(x_2,y_2)m,...,(x_N,y_N)\}$，其中 $x\in\mathcal{X}\subseteq \textbf{R}^n, y\in\mathcal{Y}=\{-1,+1\}$；弱学习算法；
- 输出：最终分类器 $G(x)$；

1. 初始化训练数据的权值分布：
   $$
   D_1=(w_{11},...,w_{1i},...,w_{1N}),\ \ \ \ \ w_{1i}=\frac{1}{N},\ \ \ i=1,2,..N
   $$

2. 对于 $m=1,2,...,M$

   - 使用具有权值分布 $D_m$ 的训练数据集学习，得到基本分类器：
     $$
     G_m(x):\mathcal{X}\rightarrow \{-1,+1\}
     $$

   - 计算 $G_m$ 在训练数据机上的分类误差率：

   $$
   e_m=P(G_m(x_i)\neq y_i)=\sum_{i=1}^Nw_{mi}I(G_m(x_i)\neq y_i)
   $$

   - 计算 $G_m(x)$ 的系数
     $$
     \alpha_m=\frac{1}{2}log\frac{1-e_m}{e_m}
     $$
     这里的对数指的是 自然对数，也就是 $ln$。

   - 更新训练数据集的权值分布：
     $$
     D_{m+1}=(w_{m+1,1},...,w_{m+1,i},...,w_{m+1,N})\\
     w_{m+1,i}=\frac{w_{mi}}{Z_m}exp(-\alpha_my_iG_m(x_i)),\ \ \ i=1,2,...,N
     $$
     这里 $Z_m$ 是规范化因子：
     $$
     Z_m = \sum_{i=1}^Nw_{mi}exp(-\alpha_my_iG_m(x_i))
     $$
     它的作用是使得 $D_{m+1}$ 成为一个概率分布

3. 构建基本分类器的线性组合

$$
f(x)=\sum_{m=1}^M\alpha_mG_m(x)
$$

从而进一步得到最终的分类器：
$$
G(x)=sign(f(x))=sign\left(\sum_{m=1}^M\alpha_mG_m(x)\right)
$$
