 # 集成学习

&emsp;&emsp; 集成学习（ensemble learning）通过构建并结合多个学习起来完成 学习任务，有时也被称为多分类器系统（multi-classifer system)、基于委员会的学习等等。

&emsp;&emsp;通过不同的方式来将多个分类器结合成为一个强分类器，具体可以分为“同质集成”和“异质集成”。其中，同质指的是最后构建的分类器系统仅仅由一种相同的 基分类器构成，而 异质 指的是最后构建的分类器系统由不仅一种基分类器构成，这时候基分类器一般也不叫基分类器，而是“组件学习器”。

&emsp;&emsp;需要说明的是，将一系列的若学习器结合在一起构成想要构成一个更强的学习器（分类器）也是有要求的，也就是说，想要获得好的集成，应当：

- 个体学习器应该有一定的 “准确性”
- 个体学习器之间应该有一定的 ”差异“

总的来说，就是要个体分类器之间做到 ”**好而不同**“。

&emsp;&emsp;但是，所有用来集成的个体学习器都是为了解决同一个问题训练得来的，因此它们显然不可能相互独立，事实上，个体学习器之间的 “准确性” 与 “多样性” 本身就存在着冲突。一般的，在准确性很高之后，想要增加多样性，就要牺牲一定的 准确性。



&emsp;&emsp;根据个体学习器的生成方式，目前的集成学习方法大致可以分成两大类：

1. 个体学习器之间存在前依赖关系，必须穿串行生成的序列化方法
2. 个体学习器之间不存在强依赖关系、可同时生成的并行化方法；

第一种的代表方法是 “boosting”，后面一中的代表方法是 “Bagging” 和 "随机森林"。



## 结合策略

&emsp;&emsp;这部分参考[集成学习原理小结](http://www.cnblogs.com/pinard/p/6131423.html)

&emsp;&emsp;假设用来进行集成学习的几个分类器分别为 $\{h_1,h_2,...,h_m\}$

1. 平均结合法：

   &emsp;&emsp;对于数值类的回归问题，通常使用的结合策略是 平均法，也就是说，对于若干和若学习器的输出进行平均得到最终的预测输出。

   - 算术平均，即为：
     $$
     H(x)=\frac{1}{T}\sum_1^Th_i(x)
     $$

   - 加权结合，假设每个学习器有一个权重 $w$

   $$
   H(x)=\sum_{i=1}^Tw_ih_i(x)
   $$

   &emsp;&emsp;其中 $w_i$ 是个体学习器 $h_i$ 的权重，通常有：
   $$
   w_i\geq0,\ \ \ \sum_{i=1}^Tw_i=1
   $$

2. 投票法：

   &emsp;&emsp;对于分类问题的预测，我们通常使用的是投票法，假设我们的预测类别是 $\{c_1,c_2,...,c_k\}$，对于任意一个预测样本 $x$，我们的 $T$ 个若学习器的预测结果是 $\{h_1(x),h_2(x),...,h_T(x)\}$。

   &emsp;&emsp;a. 相对多数投票法，也就是我们常说的少数服从多数。如果最后不止一个类别获得了最高票数，则随机选择一个做最终的类别。

   &emsp;&emsp;b. 绝对多数投票法，也就是我们常说的票数必须要过半。在上面相对多数投票法的基础上，不光要求获得最高票数，同时要求票数必须过半，否则学习器会拒绝预测。

   &emsp;&emsp;c. 加权投票法，与健全平均法一样，每个若学习器的分类票数要乘以一个权重，最终将各个来别的加权票数进行求和，最大的值对应的类别为最重类别。

3. 学习法：

   &emsp;&emsp;上面的方法都是对弱学习器的结果做平均或者投票，相对比较简单，但是可能学习误差较大，于是就有了学习法这种方法，对于学习法，代表方法是stacking，当使用stacking的结合策略时， 我们不是对弱学习器的结果做简单的逻辑处理，而是 **再加上一层学习器**，也就是说，我们**将训练集弱学习器的学习结果作为输入，将训练集的输出作为输出，重新训练一个学习器来得到最终结果**。

   &emsp;&emsp;在这种情况下，我们将弱学习器称为初级学习器，将用于结合的学习器称为次级学习器。对于测试集，我们首先用初级学习器预测一次，得到次级学习器的输入样本，再用次级学习器预测一次，得到最终的预测结果。





## Boosting

&emsp;&emsp;Boosting 算法是一族可以将若学习器提升为强学习器的算法。工作机制大致如下：

> - 首先从出事训练集训练出一个 及学习期。
> - 在根据基学习器的表现对训练样本进行调整，使得新签基学习器做错的训练样本在后续受到更多的关注
> - 基于调整后的样本分布来训练下一个基学习器
> - 如此冲进行，直到基学习器数目达到了事先指定的数目 $T$，最终将这 $T$ 个基学习器进行加权结合

上面的步骤中最后一个写到了 “进行加权结合”，这种加权结合是一种 平均结合方法。关于结合方法在上面已经进行了相关的介绍。



#### Adaboost

&emsp;&emsp; 对于所有的 Boosting 算法来说，都有两个问题需要解答：

1. 在每一轮如何改变训练数据的权值或概率分布？
2. 如何将弱分类器组合成为一个强分类器？

关于问题一，Adaboost 的做法是：

> 提高那些前一轮被错误分类样本的权值，而降低那些前一轮被正确分类的样本的权值。

关于问题二，Adaboost 的做法是：

> 采用加权多数表决的方法。也就是说，加大分类误差率小的弱分类器的权值，使其在表决中起较大的作用，减小分类误差率大的弱分类器的权值，使其在表决中起比较小的作用。  



##### Adaboost 算法

&emsp;&emsp;假设给定一个二分类的训练数据集：
$$
T=\{(x_1,y_1),(x_2,y_2)m,...,(x_N,y_N)\}
$$
其中，每个样本点由实例与标记组成。其中，实例 $x\in\mathcal{X}\subseteq \textbf{R}^n$，标记 $y\in\mathcal{Y}=\{-1,+1\}$，其中 $\mathcal{X}$ 是实例空间， $\mathcal{Y}$ 是标记集合。Adaboost 使用下面的算法，从训练数据中学习出一系列弱分类器或是基本分类器，并将这些弱分类器 **线性组合** 成为一个强分类器。

- 输入：训练数据集 $T=\{(x_1,y_1),(x_2,y_2)m,...,(x_N,y_N)\}$，其中 $x\in\mathcal{X}\subseteq \textbf{R}^n, y\in\mathcal{Y}=\{-1,+1\}$；弱学习算法；
- 输出：最终分类器 $G(x)$；

1. 初始化训练数据的权值分布：
   $$
   D_1=(w_{11},...,w_{1i},...,w_{1N}),\ \ \ \ \ w_{1i}=\frac{1}{N},\ \ \ i=1,2,..N
   $$

2. 对于 $m=1,2,...,M$

   - 使用具有权值分布 $D_m$ 的训练数据集学习，得到基本分类器：
     $$
     G_m(x):\mathcal{X}\rightarrow \{-1,+1\}
     $$

   - 计算 $G_m$ 在训练数据机上的分类误差率：

   $$
   e_m=P(G_m(x_i)\neq y_i)=\sum_{i=1}^Nw_{mi}I(G_m(x_i)\neq y_i)
   $$

   - 计算 $G_m(x)$ 的系数
     $$
     \alpha_m=\frac{1}{2}log\frac{1-e_m}{e_m}
     $$
     这里的对数指的是 自然对数，也就是 $ln$。

   - 更新训练数据集的权值分布：
     $$
     D_{m+1}=(w_{m+1,1},...,w_{m+1,i},...,w_{m+1,N})\\
     w_{m+1,i}=\frac{w_{mi}}{Z_m}exp(-\alpha_my_iG_m(x_i)),\ \ \ i=1,2,...,N
     $$
     这里 $Z_m$ 是规范化因子：
     $$
     Z_m = \sum_{i=1}^Nw_{mi}exp(-\alpha_my_iG_m(x_i))
     $$
     它的作用是使得 $D_{m+1}$ 成为一个概率分布

3. 构建基本分类器的线性组合

$$
f(x)=\sum_{m=1}^M\alpha_mG_m(x)
$$

从而进一步得到最终的分类器：
$$
G(x)=sign(f(x))=sign\left(\sum_{m=1}^M\alpha_mG_m(x)\right)
$$

#### 算法说明：

&emsp;&emsp;**步骤 1**：假设悬链数据基友均匀的权值分布，也就是每个训练样本在基本分类器的学习中作用相同，这一假设保证了第一步能够在原始数据上学习基本分类器 $G_1(x)$。

&emsp;&emsp;**步骤 2**：Adaboost 反复学习基本分类器，在每一轮 $m=1,2,...,M$ 顺序地执行下面的操作：

- 使用当前分布 $D_m$ 加权的训练数据集，学习基本分类器 $G_m(x)$.

- 计算基本分类器 $G_m(x)$ 在加权训练数据集上的分类误差率：
  $$
  e_m=P(G_m(x_i)\neq y_i)=\sum_{G_m(x_i)\neq y_i}w_{mi}
  $$
  这里，$w_{mi}$ 表示第 $m$ 轮中第 $i$ 个实例的权值，$\sum_{i=1}^{N}w_{mi}=1$。这表明，$G_m$ 在加权的训练数据集上的分类误差率是被 $G_m(x)$ 误分类样本的权值之和，由此可以看出权值分布 $D_m$ 与基本分类器 $G_m(x)$ 的分类误差率的关系。

- 计算基本分类器 $G_m(x)$ 的系数 $\alpha_m$。这个 $\alpha_m$ 表示 $G_m(x)$ 在最终分类器中的重要性。由上面的表达式可以知道，当 $e_m<\frac{1}{2}$ 时，$\alpha_m \geq 0$。并且 $\alpha_m$ 随着 $e_m$ 的减小而增大，所以分类误差率越小的基本分类器在最终分类器中的作用越大。

  给出图像：


  ![微信图片_20180312200527](C:\Users\Damu\Desktop\微信图片_20180312200527.jpg)

- 更新训练数据的取值分布为下一轮做准备，上面的更新公式可以化简为：

$$
w_{m+1,i}=\left\{ \begin{align}
\frac{w_{mi}}{Z_m}e^{-\alpha_m},\ \ \ G_m(x_i)=y_i \\
\frac{w_{mi}}{Z_m}e^{\alpha_m},\ \ \ G_m(x_i)\neq y_i
\end{align}
\right.
$$

&emsp;&emsp;从上面这个公式可以看出，被基本分类器 $G_m(x)$ 误分类样本的权值得以扩大，而被正确分类样本的权值却得以缩小。两者相互比较，误分类样本的权值被放大了 $e^{2\alpha_m}=\frac{e_m}{1-e_m}$倍，因此，误分类样本在下一轮学习中会起到更大的作用。

&emsp;&emsp;不改变所给的训练数据，而不断改变训练数据权值的分布，使得训练数据在基本分类器的学习中起不同的作用，这也正是 $Adaboost$ 的一个特点。



&emsp;&emsp;**步骤 3**：线性组合 $f(x)$ 实现 $M$ 个基本分类器的加权表决。系数 $\alpha_m$ 表示了基本分类器 $G_m(x)$ 的重要性，这里，所有的 $\alpha_m$ 之和并不为 1。其中，$f(x)$ 的符号决定实例 $x$ 的类，$f(x)$ 的绝对值标识分类的确信度。利用基本分类器的线性组合构建最终分类器是 $Adaboost$ 的另一个特点。



### 前向分步算法与 AdaBoost

&emsp;&emsp;由前向分步算法可以推导出 $Adaboost$，使用定理叙述这一关系。

&emsp;&emsp;**定理** ： Adaboost 算法是前向分步加法算法的特例。这时，模型是有基本分类器组成的加法模型，损失函数是指数函数。

&emsp;&emsp;**证明**： 前向分步算法学习的是加法模型，当基函数为基本分类器时，该加法模型等价于 $AdaBoost$ 的最终分类器：
$$
f(x)=\sum_{m=1}^N\alpha_mG_m(x)
$$
从上式可以看出，最终分类器是由基本分类器 $G_m(x)$ 以及其系数 $\alpha_m$ 组成的，其中，$m=1,2,...,M$

&emsp;&emsp;前向分步算法逐一学习基函数，这一过程与 Adaboost 算法逐一学习基本分类器的过程是一致的。下面给出证明，当前向分步算法的损失函数是**指数损失函数**的时候，其学习的具体操作等价于 $AdaBoost$ 算法学习的具体操作。其中，指数损失函数为：
$$
L(y,f(x))=exp[-yf(x)]
$$


&emsp;&emsp;假设经过 $m-1$ 轮的迭代前向分步算法已经得到了 $f_{m-1}(x)$:
$$
f_{m-1}(x)=\sum_{i=1}^{m-1}\alpha_iG_i(x)
$$
而第 $m$ 轮强学习器可以表述为：
$$
f_m(x)=\sum_{i=1}^m\alpha_iG_i(x)
$$
通过上面两个式子可以的得到：
$$
f_m(x)=f_{m-1}(x)\alpha_mG_m(x)
$$
学习的目标是使得前向分步算法得到的 $\alpha_m,G_m(x)$ 使得 $f_m(x)$ 在训练数据集 $T$ 上的指数损失最小，也就是有：
$$
(\alpha_m,G_m(x))=arg\ min_{\alpha,G}\sum_{i=1}^Nexp[-y_i(f_{m-1}x_i)+\alpha G(x_i)]
$$
上式可以表示为：
$$
(\alpha_m,G_m(x))=arg\ min_{\alpha,G}\sum_{i=1}^N\bar{w}_{mi}exp(-y_i\alpha G(x_i))
$$
其中，$\bar{w}_{mi}=exp(-y_if_{m-1}(x_i))$，因为这个项既不依赖 $\alpha$ 也不依赖 $G$，因此与最小化无关。但是 $\bar{w}_mi$ 依赖于 $f_{m-1}(x)$，会随着每一轮的迭代而改变。

&emsp;&emsp;因此现在就是要证明上式中能够达到最小的 $\alpha, G$ 就是 $Adaboost$ 算法所得到的 $\alpha_m,G_m(x)$。

- 首先，求 $G_m^*(x)$。对于任意的 $\alpha>0$，上式中最小的 $G(x)$ 由下面的式子得到：
  $$
  G_m^*(x)=arg\ min_G\sum_{i=1}^N\bar{w}_{mi}I(y_i\neq G(x_xi))
  $$
  其中，$\bar{w}_{mi}=exp[-y_if_{m-1}(x_i)]$

  这个分类器 $G_m^*(x)$ 也就是 $AdaBoost$ 算法的基本分类器 $G_m(x)$，因为它是使第 $m$ 轮家犬训练数据分类误差率最小的基本分类器

- 然后，求 $\alpha_m^*$。将上面 $G_m^*$ 的表达式带入到征途的损失中，可以得到：
  $$
  \sum_{i=1}^N\bar{w}_{mi}exp[-y_i\alpha G(x_i)]=\sum_{y=G_m(x_i)}\bar{w}_{mi}e^{-\alpha}+\sum_{y\neq G_m(x_i)}\bar{w}_{mi}e^{\alpha}\\
  =(e^{\alpha}-e^{-\alpha})\sum_{i=1}^N\bar{w}_{mi}I(y_i\neq G(x_i))+e^{-\alpha}\sum_{i=1}^N\bar{w}_{mi}
  $$

- 上式对 $\alpha$ 进行求导可得：
  $$
  \alpha_m^*=\frac{1}{2}log\frac{1-e_m}{e_m}
  $$
  其中，$e_m$ 是分类误差率：
  $$
  e_m=\frac{\sum_{i=1}^N\bar{w}_{mi}I(y_i\neq G_m(x_i))}{\sum_{i=1}^{N}\bar{w}_{mi}}=\sum_{i=1}^Nw_{mi}I(y_i\neq G_m(x_i))
  $$
  可以看到，这里的 $\alpha_m^*$ 与 Adaboost 算法中的 更新权重完全相同。



&emsp;&emsp;首先
























